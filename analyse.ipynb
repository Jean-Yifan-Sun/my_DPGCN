{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 629,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Dataset', 'Vanilla train acc', 'Vanilla train loss',\n",
       "       'Vanilla train f1', 'Vanilla test acc', 'Vanilla test loss',\n",
       "       'Vanilla test f1', 'Shadow train acc', 'Shadow train loss',\n",
       "       'Shadow train f1', 'Shadow test acc', 'Shadow test loss',\n",
       "       'Shadow test f1', 'MIA subsample rate', 'MIA mlp', 'MIA svm',\n",
       "       'MIA ranfor', 'MIA logi', 'MIA ada', 'MIA confidence mse',\n",
       "       'MIA confidence thr', 'MIA seed', 'Vanilla runtime per',\n",
       "       'Shadow runtime per', 'Epsilon', 'Delta', 'Dp', 'Rdp', 'Ldp',\n",
       "       'Norm bound', 'Noise scale', 'Sampler', 'Sampler batchsize',\n",
       "       'Occurance k', 'Cluster numparts', 'Saint rootnodes',\n",
       "       'Saint samplecoverage', 'Saint walklenth', 'Epochs', 'Shadow epochs',\n",
       "       'Num val', 'Num test', 'Layers', 'Hidden dims', 'Learning rate',\n",
       "       'Shadow learning rate', 'Dropout', 'Activation', 'Early stopping',\n",
       "       'Patience', 'Optim type'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 629,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from torch_geometric.data import Data,Dataset\n",
    "\n",
    "total_parms_new = pd.read_csv('/mnt/ssd1/sunyifan/WorkStation/dpuf/my_DPGCN/total_params_new.csv',index_col=0)\n",
    "total_parms = pd.read_csv('/mnt/ssd1/sunyifan/WorkStation/dpuf/my_DPGCN/total_params.csv',index_col=0)\n",
    "\n",
    "from typing import List, Literal, Sequence, Union\n",
    "\n",
    "\n",
    "dataset_list = ['cora','citeseer','pubmed','computers','photo','cs','physics','reddit','github','flickr','lastfmasia']\n",
    "def get_datasetwise_df(df:pd.DataFrame,name=None):\n",
    "    if not name:\n",
    "        dflist = []\n",
    "        for i in ['cora','citeseer','pubmed','computers','photo','cs','physics','reddit','github','flickr','lastfmasia']:\n",
    "            dflist.append(df[df['Dataset']==i])\n",
    "        return dflist\n",
    "    else:\n",
    "        return df[df['Dataset']==name]\n",
    "\n",
    "def get_f1accs(df: Union[pd.DataFrame, List[pd.DataFrame]]):\n",
    "    metrics = ['Vanilla test acc','Vanilla test f1','Shadow test acc','Shadow test f1','MIA mlp', 'MIA svm',\n",
    "       'MIA ranfor', 'MIA logi', 'MIA ada', 'MIA confidence mse','Vanilla runtime per']\n",
    "    if isinstance(df,list):\n",
    "        res = []\n",
    "        for i in df:\n",
    "            res.append(i[metrics].max())\n",
    "        return res\n",
    "    else:\n",
    "        return df[metrics].max()\n",
    "    \n",
    "\n",
    "total_parms_new.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 630,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline(df:pd.DataFrame):\n",
    "    clean_adam = df[(df['Dp']==False)&(df['Rdp']==False)&(df['Ldp']==False)&(df['Optim type']=='adam')]\n",
    "    clean_sgd = df[(df['Dp']==False)&(df['Rdp']==False)&(df['Ldp']==False)&(df['Optim type']=='sgd')]\n",
    "    return clean_adam,clean_sgd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 631,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_adam = total_parms[(total_parms['Dp']==False)&(total_parms['Rdp']==False)&(total_parms['Ldp']==False)&(total_parms['Optim type']=='adam')]\n",
    "clean_sgd = total_parms[(total_parms['Dp']==False)&(total_parms['Rdp']==False)&(total_parms['Ldp']==False)&(total_parms['Optim type']=='sgd')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 632,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Vanilla train acc</th>\n",
       "      <th>Vanilla train loss</th>\n",
       "      <th>Vanilla train f1</th>\n",
       "      <th>Vanilla test acc</th>\n",
       "      <th>Vanilla test loss</th>\n",
       "      <th>Vanilla test f1</th>\n",
       "      <th>Shadow train acc</th>\n",
       "      <th>Shadow train loss</th>\n",
       "      <th>Shadow train f1</th>\n",
       "      <th>...</th>\n",
       "      <th>Num test</th>\n",
       "      <th>Layers</th>\n",
       "      <th>Hidden dims</th>\n",
       "      <th>Learning rate</th>\n",
       "      <th>Shadow learning rate</th>\n",
       "      <th>Dropout</th>\n",
       "      <th>Activation</th>\n",
       "      <th>Early stopping</th>\n",
       "      <th>Patience</th>\n",
       "      <th>Optim type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows Ã— 49 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Dataset, Vanilla train acc, Vanilla train loss, Vanilla train f1, Vanilla test acc, Vanilla test loss, Vanilla test f1, Shadow train acc, Shadow train loss, Shadow train f1, Shadow test acc, Shadow test loss, Shadow test f1, MIA subsample rate, MIA mlp, MIA svm, MIA ranfor, MIA logi, MIA ada, MIA confidence mse, MIA confidence thr, MIA seed, Vanilla runtime per, Shadow runtime per, Epsilon, Delta, Dp, Rdp, Ldp, Sampler, Sampler batchsize, Occurance k, Cluster numparts, Saint rootnodes, Saint samplecoverage, Saint walklenth, Epochs, Shadow epochs, Num val, Num test, Layers, Hidden dims, Learning rate, Shadow learning rate, Dropout, Activation, Early stopping, Patience, Optim type]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 49 columns]"
      ]
     },
     "execution_count": 632,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_sgd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 633,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,clean_sgd = baseline(total_parms_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 634,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Vanilla train acc</th>\n",
       "      <th>Vanilla train loss</th>\n",
       "      <th>Vanilla train f1</th>\n",
       "      <th>Vanilla test acc</th>\n",
       "      <th>Vanilla test loss</th>\n",
       "      <th>Vanilla test f1</th>\n",
       "      <th>Shadow train acc</th>\n",
       "      <th>Shadow train loss</th>\n",
       "      <th>Shadow train f1</th>\n",
       "      <th>...</th>\n",
       "      <th>Num test</th>\n",
       "      <th>Layers</th>\n",
       "      <th>Hidden dims</th>\n",
       "      <th>Learning rate</th>\n",
       "      <th>Shadow learning rate</th>\n",
       "      <th>Dropout</th>\n",
       "      <th>Activation</th>\n",
       "      <th>Early stopping</th>\n",
       "      <th>Patience</th>\n",
       "      <th>Optim type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>510</th>\n",
       "      <td>cora</td>\n",
       "      <td>0.8933</td>\n",
       "      <td>0.5738</td>\n",
       "      <td>0.8933</td>\n",
       "      <td>0.7800</td>\n",
       "      <td>0.8644</td>\n",
       "      <td>0.7800</td>\n",
       "      <td>0.8916</td>\n",
       "      <td>0.6144</td>\n",
       "      <td>0.8916</td>\n",
       "      <td>...</td>\n",
       "      <td>0.45</td>\n",
       "      <td>2</td>\n",
       "      <td>256</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.5</td>\n",
       "      <td>relu</td>\n",
       "      <td>False</td>\n",
       "      <td>100</td>\n",
       "      <td>sgd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>511</th>\n",
       "      <td>cora</td>\n",
       "      <td>0.9031</td>\n",
       "      <td>0.6010</td>\n",
       "      <td>0.9031</td>\n",
       "      <td>0.8013</td>\n",
       "      <td>0.7987</td>\n",
       "      <td>0.8013</td>\n",
       "      <td>0.8916</td>\n",
       "      <td>0.5872</td>\n",
       "      <td>0.8916</td>\n",
       "      <td>...</td>\n",
       "      <td>0.45</td>\n",
       "      <td>2</td>\n",
       "      <td>256</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.5</td>\n",
       "      <td>relu</td>\n",
       "      <td>False</td>\n",
       "      <td>100</td>\n",
       "      <td>sgd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>512</th>\n",
       "      <td>cora</td>\n",
       "      <td>0.8785</td>\n",
       "      <td>0.6489</td>\n",
       "      <td>0.8785</td>\n",
       "      <td>0.7931</td>\n",
       "      <td>0.8245</td>\n",
       "      <td>0.7931</td>\n",
       "      <td>0.8736</td>\n",
       "      <td>0.6153</td>\n",
       "      <td>0.8736</td>\n",
       "      <td>...</td>\n",
       "      <td>0.45</td>\n",
       "      <td>2</td>\n",
       "      <td>256</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.5</td>\n",
       "      <td>relu</td>\n",
       "      <td>False</td>\n",
       "      <td>100</td>\n",
       "      <td>sgd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>513</th>\n",
       "      <td>cora</td>\n",
       "      <td>0.8818</td>\n",
       "      <td>0.6372</td>\n",
       "      <td>0.8818</td>\n",
       "      <td>0.7898</td>\n",
       "      <td>0.8272</td>\n",
       "      <td>0.7898</td>\n",
       "      <td>0.9015</td>\n",
       "      <td>0.5717</td>\n",
       "      <td>0.9015</td>\n",
       "      <td>...</td>\n",
       "      <td>0.45</td>\n",
       "      <td>2</td>\n",
       "      <td>256</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.5</td>\n",
       "      <td>relu</td>\n",
       "      <td>False</td>\n",
       "      <td>100</td>\n",
       "      <td>sgd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>514</th>\n",
       "      <td>cora</td>\n",
       "      <td>0.8851</td>\n",
       "      <td>0.6160</td>\n",
       "      <td>0.8851</td>\n",
       "      <td>0.8079</td>\n",
       "      <td>0.8025</td>\n",
       "      <td>0.8079</td>\n",
       "      <td>0.8785</td>\n",
       "      <td>0.6212</td>\n",
       "      <td>0.8785</td>\n",
       "      <td>...</td>\n",
       "      <td>0.45</td>\n",
       "      <td>2</td>\n",
       "      <td>256</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.5</td>\n",
       "      <td>relu</td>\n",
       "      <td>False</td>\n",
       "      <td>100</td>\n",
       "      <td>sgd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>852</th>\n",
       "      <td>flickr</td>\n",
       "      <td>0.5112</td>\n",
       "      <td>1.3820</td>\n",
       "      <td>0.5112</td>\n",
       "      <td>0.4486</td>\n",
       "      <td>1.5790</td>\n",
       "      <td>0.4486</td>\n",
       "      <td>0.5131</td>\n",
       "      <td>1.3889</td>\n",
       "      <td>0.5131</td>\n",
       "      <td>...</td>\n",
       "      <td>0.45</td>\n",
       "      <td>2</td>\n",
       "      <td>256</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.5</td>\n",
       "      <td>relu</td>\n",
       "      <td>False</td>\n",
       "      <td>100</td>\n",
       "      <td>sgd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>853</th>\n",
       "      <td>flickr</td>\n",
       "      <td>0.5144</td>\n",
       "      <td>1.3810</td>\n",
       "      <td>0.5144</td>\n",
       "      <td>0.4596</td>\n",
       "      <td>1.5552</td>\n",
       "      <td>0.4596</td>\n",
       "      <td>0.5182</td>\n",
       "      <td>1.3805</td>\n",
       "      <td>0.5182</td>\n",
       "      <td>...</td>\n",
       "      <td>0.45</td>\n",
       "      <td>2</td>\n",
       "      <td>256</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.5</td>\n",
       "      <td>relu</td>\n",
       "      <td>False</td>\n",
       "      <td>100</td>\n",
       "      <td>sgd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>854</th>\n",
       "      <td>flickr</td>\n",
       "      <td>0.5118</td>\n",
       "      <td>1.3960</td>\n",
       "      <td>0.5118</td>\n",
       "      <td>0.4561</td>\n",
       "      <td>1.5663</td>\n",
       "      <td>0.4561</td>\n",
       "      <td>0.5117</td>\n",
       "      <td>1.4054</td>\n",
       "      <td>0.5117</td>\n",
       "      <td>...</td>\n",
       "      <td>0.45</td>\n",
       "      <td>2</td>\n",
       "      <td>256</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.5</td>\n",
       "      <td>relu</td>\n",
       "      <td>False</td>\n",
       "      <td>100</td>\n",
       "      <td>sgd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>855</th>\n",
       "      <td>flickr</td>\n",
       "      <td>0.5235</td>\n",
       "      <td>1.3671</td>\n",
       "      <td>0.5235</td>\n",
       "      <td>0.4555</td>\n",
       "      <td>1.5888</td>\n",
       "      <td>0.4555</td>\n",
       "      <td>0.5078</td>\n",
       "      <td>1.3890</td>\n",
       "      <td>0.5078</td>\n",
       "      <td>...</td>\n",
       "      <td>0.45</td>\n",
       "      <td>2</td>\n",
       "      <td>256</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.5</td>\n",
       "      <td>relu</td>\n",
       "      <td>False</td>\n",
       "      <td>100</td>\n",
       "      <td>sgd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860</th>\n",
       "      <td>lastfmasia</td>\n",
       "      <td>0.8021</td>\n",
       "      <td>0.8369</td>\n",
       "      <td>0.8021</td>\n",
       "      <td>0.7765</td>\n",
       "      <td>0.9566</td>\n",
       "      <td>0.7765</td>\n",
       "      <td>0.7992</td>\n",
       "      <td>0.8636</td>\n",
       "      <td>0.7992</td>\n",
       "      <td>...</td>\n",
       "      <td>0.45</td>\n",
       "      <td>2</td>\n",
       "      <td>256</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.5</td>\n",
       "      <td>relu</td>\n",
       "      <td>False</td>\n",
       "      <td>200</td>\n",
       "      <td>sgd</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>101 rows Ã— 51 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Dataset  Vanilla train acc  Vanilla train loss  Vanilla train f1  \\\n",
       "510        cora             0.8933              0.5738            0.8933   \n",
       "511        cora             0.9031              0.6010            0.9031   \n",
       "512        cora             0.8785              0.6489            0.8785   \n",
       "513        cora             0.8818              0.6372            0.8818   \n",
       "514        cora             0.8851              0.6160            0.8851   \n",
       "..          ...                ...                 ...               ...   \n",
       "852      flickr             0.5112              1.3820            0.5112   \n",
       "853      flickr             0.5144              1.3810            0.5144   \n",
       "854      flickr             0.5118              1.3960            0.5118   \n",
       "855      flickr             0.5235              1.3671            0.5235   \n",
       "860  lastfmasia             0.8021              0.8369            0.8021   \n",
       "\n",
       "     Vanilla test acc  Vanilla test loss  Vanilla test f1  Shadow train acc  \\\n",
       "510            0.7800             0.8644           0.7800            0.8916   \n",
       "511            0.8013             0.7987           0.8013            0.8916   \n",
       "512            0.7931             0.8245           0.7931            0.8736   \n",
       "513            0.7898             0.8272           0.7898            0.9015   \n",
       "514            0.8079             0.8025           0.8079            0.8785   \n",
       "..                ...                ...              ...               ...   \n",
       "852            0.4486             1.5790           0.4486            0.5131   \n",
       "853            0.4596             1.5552           0.4596            0.5182   \n",
       "854            0.4561             1.5663           0.4561            0.5117   \n",
       "855            0.4555             1.5888           0.4555            0.5078   \n",
       "860            0.7765             0.9566           0.7765            0.7992   \n",
       "\n",
       "     Shadow train loss  Shadow train f1  ...  Num test  Layers  Hidden dims  \\\n",
       "510             0.6144           0.8916  ...      0.45       2          256   \n",
       "511             0.5872           0.8916  ...      0.45       2          256   \n",
       "512             0.6153           0.8736  ...      0.45       2          256   \n",
       "513             0.5717           0.9015  ...      0.45       2          256   \n",
       "514             0.6212           0.8785  ...      0.45       2          256   \n",
       "..                 ...              ...  ...       ...     ...          ...   \n",
       "852             1.3889           0.5131  ...      0.45       2          256   \n",
       "853             1.3805           0.5182  ...      0.45       2          256   \n",
       "854             1.4054           0.5117  ...      0.45       2          256   \n",
       "855             1.3890           0.5078  ...      0.45       2          256   \n",
       "860             0.8636           0.7992  ...      0.45       2          256   \n",
       "\n",
       "     Learning rate  Shadow learning rate  Dropout  Activation  Early stopping  \\\n",
       "510           0.02                  0.02      0.5        relu           False   \n",
       "511           0.02                  0.02      0.5        relu           False   \n",
       "512           0.02                  0.02      0.5        relu           False   \n",
       "513           0.02                  0.02      0.5        relu           False   \n",
       "514           0.02                  0.02      0.5        relu           False   \n",
       "..             ...                   ...      ...         ...             ...   \n",
       "852           0.02                  0.02      0.5        relu           False   \n",
       "853           0.02                  0.02      0.5        relu           False   \n",
       "854           0.02                  0.02      0.5        relu           False   \n",
       "855           0.02                  0.02      0.5        relu           False   \n",
       "860           0.02                  0.02      0.5        relu           False   \n",
       "\n",
       "     Patience  Optim type  \n",
       "510       100         sgd  \n",
       "511       100         sgd  \n",
       "512       100         sgd  \n",
       "513       100         sgd  \n",
       "514       100         sgd  \n",
       "..        ...         ...  \n",
       "852       100         sgd  \n",
       "853       100         sgd  \n",
       "854       100         sgd  \n",
       "855       100         sgd  \n",
       "860       200         sgd  \n",
       "\n",
       "[101 rows x 51 columns]"
      ]
     },
     "execution_count": 634,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_sgd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 635,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Vanilla test acc</th>\n",
       "      <th>Vanilla test f1</th>\n",
       "      <th>Shadow test acc</th>\n",
       "      <th>Shadow test f1</th>\n",
       "      <th>MIA mlp</th>\n",
       "      <th>MIA svm</th>\n",
       "      <th>MIA ranfor</th>\n",
       "      <th>MIA logi</th>\n",
       "      <th>MIA ada</th>\n",
       "      <th>MIA confidence mse</th>\n",
       "      <th>Vanilla runtime per</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>cora</th>\n",
       "      <td>0.8112</td>\n",
       "      <td>0.8112</td>\n",
       "      <td>0.8128</td>\n",
       "      <td>0.8128</td>\n",
       "      <td>0.5821</td>\n",
       "      <td>0.5936</td>\n",
       "      <td>0.5665</td>\n",
       "      <td>0.5468</td>\n",
       "      <td>0.5698</td>\n",
       "      <td>0.6026</td>\n",
       "      <td>0.1845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>citeseer</th>\n",
       "      <td>0.7219</td>\n",
       "      <td>0.7219</td>\n",
       "      <td>0.7380</td>\n",
       "      <td>0.7380</td>\n",
       "      <td>0.5926</td>\n",
       "      <td>0.5813</td>\n",
       "      <td>0.5739</td>\n",
       "      <td>0.5505</td>\n",
       "      <td>0.5764</td>\n",
       "      <td>0.5952</td>\n",
       "      <td>0.1805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pubmed</th>\n",
       "      <td>0.6105</td>\n",
       "      <td>0.6105</td>\n",
       "      <td>0.5940</td>\n",
       "      <td>0.5940</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.5050</td>\n",
       "      <td>0.5187</td>\n",
       "      <td>0.5108</td>\n",
       "      <td>0.5074</td>\n",
       "      <td>0.5123</td>\n",
       "      <td>0.1903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>computers</th>\n",
       "      <td>0.8251</td>\n",
       "      <td>0.8251</td>\n",
       "      <td>0.8229</td>\n",
       "      <td>0.8229</td>\n",
       "      <td>0.5130</td>\n",
       "      <td>0.5158</td>\n",
       "      <td>0.5112</td>\n",
       "      <td>0.5193</td>\n",
       "      <td>0.5143</td>\n",
       "      <td>0.5192</td>\n",
       "      <td>0.1857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>photo</th>\n",
       "      <td>0.9140</td>\n",
       "      <td>0.9140</td>\n",
       "      <td>0.9198</td>\n",
       "      <td>0.9198</td>\n",
       "      <td>0.5192</td>\n",
       "      <td>0.5192</td>\n",
       "      <td>0.5186</td>\n",
       "      <td>0.5221</td>\n",
       "      <td>0.5180</td>\n",
       "      <td>0.5299</td>\n",
       "      <td>0.1812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cs</th>\n",
       "      <td>0.8312</td>\n",
       "      <td>0.8312</td>\n",
       "      <td>0.8303</td>\n",
       "      <td>0.8303</td>\n",
       "      <td>0.5155</td>\n",
       "      <td>0.5170</td>\n",
       "      <td>0.5093</td>\n",
       "      <td>0.5171</td>\n",
       "      <td>0.5138</td>\n",
       "      <td>0.5139</td>\n",
       "      <td>0.1938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>physics</th>\n",
       "      <td>0.9343</td>\n",
       "      <td>0.9343</td>\n",
       "      <td>0.9305</td>\n",
       "      <td>0.9305</td>\n",
       "      <td>0.5103</td>\n",
       "      <td>0.5132</td>\n",
       "      <td>0.5054</td>\n",
       "      <td>0.5129</td>\n",
       "      <td>0.5096</td>\n",
       "      <td>0.5078</td>\n",
       "      <td>0.1977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reddit</th>\n",
       "      <td>0.6953</td>\n",
       "      <td>0.6953</td>\n",
       "      <td>0.6897</td>\n",
       "      <td>0.6897</td>\n",
       "      <td>0.5077</td>\n",
       "      <td>0.5141</td>\n",
       "      <td>0.5081</td>\n",
       "      <td>0.5121</td>\n",
       "      <td>0.5112</td>\n",
       "      <td>0.5131</td>\n",
       "      <td>0.2187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>github</th>\n",
       "      <td>0.8582</td>\n",
       "      <td>0.8582</td>\n",
       "      <td>0.8592</td>\n",
       "      <td>0.8592</td>\n",
       "      <td>0.5058</td>\n",
       "      <td>0.5083</td>\n",
       "      <td>0.5086</td>\n",
       "      <td>0.5068</td>\n",
       "      <td>0.5093</td>\n",
       "      <td>0.5107</td>\n",
       "      <td>0.1815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>flickr</th>\n",
       "      <td>0.4596</td>\n",
       "      <td>0.4596</td>\n",
       "      <td>0.4668</td>\n",
       "      <td>0.4668</td>\n",
       "      <td>0.5059</td>\n",
       "      <td>0.5125</td>\n",
       "      <td>0.5060</td>\n",
       "      <td>0.5068</td>\n",
       "      <td>0.5095</td>\n",
       "      <td>0.5370</td>\n",
       "      <td>0.1908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lastfmasia</th>\n",
       "      <td>0.7765</td>\n",
       "      <td>0.7765</td>\n",
       "      <td>0.7923</td>\n",
       "      <td>0.7923</td>\n",
       "      <td>0.5001</td>\n",
       "      <td>0.4995</td>\n",
       "      <td>0.5019</td>\n",
       "      <td>0.5124</td>\n",
       "      <td>0.5028</td>\n",
       "      <td>0.5238</td>\n",
       "      <td>0.1777</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Vanilla test acc  Vanilla test f1  Shadow test acc  \\\n",
       "cora                  0.8112           0.8112           0.8128   \n",
       "citeseer              0.7219           0.7219           0.7380   \n",
       "pubmed                0.6105           0.6105           0.5940   \n",
       "computers             0.8251           0.8251           0.8229   \n",
       "photo                 0.9140           0.9140           0.9198   \n",
       "cs                    0.8312           0.8312           0.8303   \n",
       "physics               0.9343           0.9343           0.9305   \n",
       "reddit                0.6953           0.6953           0.6897   \n",
       "github                0.8582           0.8582           0.8592   \n",
       "flickr                0.4596           0.4596           0.4668   \n",
       "lastfmasia            0.7765           0.7765           0.7923   \n",
       "\n",
       "            Shadow test f1  MIA mlp  MIA svm  MIA ranfor  MIA logi  MIA ada  \\\n",
       "cora                0.8128   0.5821   0.5936      0.5665    0.5468   0.5698   \n",
       "citeseer            0.7380   0.5926   0.5813      0.5739    0.5505   0.5764   \n",
       "pubmed              0.5940   0.5000   0.5050      0.5187    0.5108   0.5074   \n",
       "computers           0.8229   0.5130   0.5158      0.5112    0.5193   0.5143   \n",
       "photo               0.9198   0.5192   0.5192      0.5186    0.5221   0.5180   \n",
       "cs                  0.8303   0.5155   0.5170      0.5093    0.5171   0.5138   \n",
       "physics             0.9305   0.5103   0.5132      0.5054    0.5129   0.5096   \n",
       "reddit              0.6897   0.5077   0.5141      0.5081    0.5121   0.5112   \n",
       "github              0.8592   0.5058   0.5083      0.5086    0.5068   0.5093   \n",
       "flickr              0.4668   0.5059   0.5125      0.5060    0.5068   0.5095   \n",
       "lastfmasia          0.7923   0.5001   0.4995      0.5019    0.5124   0.5028   \n",
       "\n",
       "            MIA confidence mse  Vanilla runtime per  \n",
       "cora                    0.6026               0.1845  \n",
       "citeseer                0.5952               0.1805  \n",
       "pubmed                  0.5123               0.1903  \n",
       "computers               0.5192               0.1857  \n",
       "photo                   0.5299               0.1812  \n",
       "cs                      0.5139               0.1938  \n",
       "physics                 0.5078               0.1977  \n",
       "reddit                  0.5131               0.2187  \n",
       "github                  0.5107               0.1815  \n",
       "flickr                  0.5370               0.1908  \n",
       "lastfmasia              0.5238               0.1777  "
      ]
     },
     "execution_count": 635,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_list = get_datasetwise_df(clean_sgd)\n",
    "\n",
    "clean_res_dfs = get_f1accs(clean_list)\n",
    "clean_res = {}\n",
    "for i in range(len(dataset_list)):\n",
    "    clean_res[dataset_list[i]] = clean_res_dfs[i].to_dict()\n",
    "pd.DataFrame(clean_res).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 637,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Vanilla train acc</th>\n",
       "      <th>Vanilla train loss</th>\n",
       "      <th>Vanilla train f1</th>\n",
       "      <th>Vanilla test acc</th>\n",
       "      <th>Vanilla test loss</th>\n",
       "      <th>Vanilla test f1</th>\n",
       "      <th>Shadow train acc</th>\n",
       "      <th>Shadow train loss</th>\n",
       "      <th>Shadow train f1</th>\n",
       "      <th>...</th>\n",
       "      <th>Num test</th>\n",
       "      <th>Layers</th>\n",
       "      <th>Hidden dims</th>\n",
       "      <th>Learning rate</th>\n",
       "      <th>Shadow learning rate</th>\n",
       "      <th>Dropout</th>\n",
       "      <th>Activation</th>\n",
       "      <th>Early stopping</th>\n",
       "      <th>Patience</th>\n",
       "      <th>Optim type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cora</td>\n",
       "      <td>0.8670</td>\n",
       "      <td>0.5521</td>\n",
       "      <td>0.8670</td>\n",
       "      <td>0.7094</td>\n",
       "      <td>0.9923</td>\n",
       "      <td>0.7094</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0125</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.45</td>\n",
       "      <td>2</td>\n",
       "      <td>256</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.5</td>\n",
       "      <td>relu</td>\n",
       "      <td>True</td>\n",
       "      <td>200</td>\n",
       "      <td>adam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cora</td>\n",
       "      <td>0.8522</td>\n",
       "      <td>0.6419</td>\n",
       "      <td>0.8522</td>\n",
       "      <td>0.7028</td>\n",
       "      <td>1.0155</td>\n",
       "      <td>0.7028</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0125</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.45</td>\n",
       "      <td>2</td>\n",
       "      <td>256</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.5</td>\n",
       "      <td>relu</td>\n",
       "      <td>True</td>\n",
       "      <td>200</td>\n",
       "      <td>adam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cora</td>\n",
       "      <td>0.8489</td>\n",
       "      <td>0.7091</td>\n",
       "      <td>0.8489</td>\n",
       "      <td>0.7291</td>\n",
       "      <td>0.9462</td>\n",
       "      <td>0.7291</td>\n",
       "      <td>0.9984</td>\n",
       "      <td>0.0128</td>\n",
       "      <td>0.9984</td>\n",
       "      <td>...</td>\n",
       "      <td>0.45</td>\n",
       "      <td>2</td>\n",
       "      <td>256</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.5</td>\n",
       "      <td>relu</td>\n",
       "      <td>True</td>\n",
       "      <td>200</td>\n",
       "      <td>adam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cora</td>\n",
       "      <td>0.8177</td>\n",
       "      <td>0.7117</td>\n",
       "      <td>0.8177</td>\n",
       "      <td>0.7011</td>\n",
       "      <td>0.9737</td>\n",
       "      <td>0.7011</td>\n",
       "      <td>0.9901</td>\n",
       "      <td>0.0271</td>\n",
       "      <td>0.9901</td>\n",
       "      <td>...</td>\n",
       "      <td>0.45</td>\n",
       "      <td>2</td>\n",
       "      <td>256</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.5</td>\n",
       "      <td>relu</td>\n",
       "      <td>True</td>\n",
       "      <td>200</td>\n",
       "      <td>adam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cora</td>\n",
       "      <td>0.8637</td>\n",
       "      <td>0.6601</td>\n",
       "      <td>0.8637</td>\n",
       "      <td>0.7521</td>\n",
       "      <td>0.9392</td>\n",
       "      <td>0.7521</td>\n",
       "      <td>0.9984</td>\n",
       "      <td>0.0102</td>\n",
       "      <td>0.9984</td>\n",
       "      <td>...</td>\n",
       "      <td>0.45</td>\n",
       "      <td>2</td>\n",
       "      <td>256</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.5</td>\n",
       "      <td>relu</td>\n",
       "      <td>True</td>\n",
       "      <td>200</td>\n",
       "      <td>adam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>cora</td>\n",
       "      <td>0.8522</td>\n",
       "      <td>0.6768</td>\n",
       "      <td>0.8522</td>\n",
       "      <td>0.7274</td>\n",
       "      <td>0.9609</td>\n",
       "      <td>0.7274</td>\n",
       "      <td>0.9967</td>\n",
       "      <td>0.0167</td>\n",
       "      <td>0.9967</td>\n",
       "      <td>...</td>\n",
       "      <td>0.45</td>\n",
       "      <td>2</td>\n",
       "      <td>256</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.5</td>\n",
       "      <td>relu</td>\n",
       "      <td>True</td>\n",
       "      <td>200</td>\n",
       "      <td>adam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>cora</td>\n",
       "      <td>0.8489</td>\n",
       "      <td>0.6410</td>\n",
       "      <td>0.8489</td>\n",
       "      <td>0.7094</td>\n",
       "      <td>0.9636</td>\n",
       "      <td>0.7094</td>\n",
       "      <td>0.9984</td>\n",
       "      <td>0.0133</td>\n",
       "      <td>0.9984</td>\n",
       "      <td>...</td>\n",
       "      <td>0.45</td>\n",
       "      <td>2</td>\n",
       "      <td>256</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.5</td>\n",
       "      <td>relu</td>\n",
       "      <td>True</td>\n",
       "      <td>200</td>\n",
       "      <td>adam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>cora</td>\n",
       "      <td>0.8440</td>\n",
       "      <td>0.6866</td>\n",
       "      <td>0.8440</td>\n",
       "      <td>0.7110</td>\n",
       "      <td>0.9748</td>\n",
       "      <td>0.7110</td>\n",
       "      <td>0.9984</td>\n",
       "      <td>0.0144</td>\n",
       "      <td>0.9984</td>\n",
       "      <td>...</td>\n",
       "      <td>0.45</td>\n",
       "      <td>2</td>\n",
       "      <td>256</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.5</td>\n",
       "      <td>relu</td>\n",
       "      <td>True</td>\n",
       "      <td>200</td>\n",
       "      <td>adam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>cora</td>\n",
       "      <td>0.8079</td>\n",
       "      <td>0.7155</td>\n",
       "      <td>0.8079</td>\n",
       "      <td>0.6585</td>\n",
       "      <td>1.0695</td>\n",
       "      <td>0.6585</td>\n",
       "      <td>0.9967</td>\n",
       "      <td>0.0127</td>\n",
       "      <td>0.9967</td>\n",
       "      <td>...</td>\n",
       "      <td>0.45</td>\n",
       "      <td>2</td>\n",
       "      <td>256</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.5</td>\n",
       "      <td>relu</td>\n",
       "      <td>True</td>\n",
       "      <td>200</td>\n",
       "      <td>adam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>cora</td>\n",
       "      <td>0.8522</td>\n",
       "      <td>0.6521</td>\n",
       "      <td>0.8522</td>\n",
       "      <td>0.7209</td>\n",
       "      <td>0.9416</td>\n",
       "      <td>0.7209</td>\n",
       "      <td>0.9967</td>\n",
       "      <td>0.0166</td>\n",
       "      <td>0.9967</td>\n",
       "      <td>...</td>\n",
       "      <td>0.45</td>\n",
       "      <td>2</td>\n",
       "      <td>256</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.5</td>\n",
       "      <td>relu</td>\n",
       "      <td>True</td>\n",
       "      <td>200</td>\n",
       "      <td>adam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>cora</td>\n",
       "      <td>0.8424</td>\n",
       "      <td>0.6635</td>\n",
       "      <td>0.8424</td>\n",
       "      <td>0.7094</td>\n",
       "      <td>0.9736</td>\n",
       "      <td>0.7094</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0084</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.45</td>\n",
       "      <td>2</td>\n",
       "      <td>256</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.5</td>\n",
       "      <td>relu</td>\n",
       "      <td>True</td>\n",
       "      <td>200</td>\n",
       "      <td>adam</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11 rows Ã— 51 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Dataset  Vanilla train acc  Vanilla train loss  Vanilla train f1  \\\n",
       "0     cora             0.8670              0.5521            0.8670   \n",
       "1     cora             0.8522              0.6419            0.8522   \n",
       "2     cora             0.8489              0.7091            0.8489   \n",
       "3     cora             0.8177              0.7117            0.8177   \n",
       "4     cora             0.8637              0.6601            0.8637   \n",
       "5     cora             0.8522              0.6768            0.8522   \n",
       "6     cora             0.8489              0.6410            0.8489   \n",
       "7     cora             0.8440              0.6866            0.8440   \n",
       "8     cora             0.8079              0.7155            0.8079   \n",
       "9     cora             0.8522              0.6521            0.8522   \n",
       "10    cora             0.8424              0.6635            0.8424   \n",
       "\n",
       "    Vanilla test acc  Vanilla test loss  Vanilla test f1  Shadow train acc  \\\n",
       "0             0.7094             0.9923           0.7094            1.0000   \n",
       "1             0.7028             1.0155           0.7028            1.0000   \n",
       "2             0.7291             0.9462           0.7291            0.9984   \n",
       "3             0.7011             0.9737           0.7011            0.9901   \n",
       "4             0.7521             0.9392           0.7521            0.9984   \n",
       "5             0.7274             0.9609           0.7274            0.9967   \n",
       "6             0.7094             0.9636           0.7094            0.9984   \n",
       "7             0.7110             0.9748           0.7110            0.9984   \n",
       "8             0.6585             1.0695           0.6585            0.9967   \n",
       "9             0.7209             0.9416           0.7209            0.9967   \n",
       "10            0.7094             0.9736           0.7094            1.0000   \n",
       "\n",
       "    Shadow train loss  Shadow train f1  ...  Num test  Layers  Hidden dims  \\\n",
       "0              0.0125           1.0000  ...      0.45       2          256   \n",
       "1              0.0125           1.0000  ...      0.45       2          256   \n",
       "2              0.0128           0.9984  ...      0.45       2          256   \n",
       "3              0.0271           0.9901  ...      0.45       2          256   \n",
       "4              0.0102           0.9984  ...      0.45       2          256   \n",
       "5              0.0167           0.9967  ...      0.45       2          256   \n",
       "6              0.0133           0.9984  ...      0.45       2          256   \n",
       "7              0.0144           0.9984  ...      0.45       2          256   \n",
       "8              0.0127           0.9967  ...      0.45       2          256   \n",
       "9              0.0166           0.9967  ...      0.45       2          256   \n",
       "10             0.0084           1.0000  ...      0.45       2          256   \n",
       "\n",
       "    Learning rate  Shadow learning rate  Dropout  Activation  Early stopping  \\\n",
       "0           0.001                 0.001      0.5        relu            True   \n",
       "1           0.001                 0.001      0.5        relu            True   \n",
       "2           0.001                 0.001      0.5        relu            True   \n",
       "3           0.001                 0.001      0.5        relu            True   \n",
       "4           0.001                 0.001      0.5        relu            True   \n",
       "5           0.001                 0.001      0.5        relu            True   \n",
       "6           0.001                 0.001      0.5        relu            True   \n",
       "7           0.001                 0.001      0.5        relu            True   \n",
       "8           0.001                 0.001      0.5        relu            True   \n",
       "9           0.001                 0.001      0.5        relu            True   \n",
       "10          0.001                 0.001      0.5        relu            True   \n",
       "\n",
       "    Patience  Optim type  \n",
       "0        200        adam  \n",
       "1        200        adam  \n",
       "2        200        adam  \n",
       "3        200        adam  \n",
       "4        200        adam  \n",
       "5        200        adam  \n",
       "6        200        adam  \n",
       "7        200        adam  \n",
       "8        200        adam  \n",
       "9        200        adam  \n",
       "10       200        adam  \n",
       "\n",
       "[11 rows x 51 columns]"
      ]
     },
     "execution_count": 637,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_res_rdp(cora:pd.DataFrame):\n",
    "    maxx = ['Vanilla test acc','Vanilla test f1']\n",
    "    meann = ['Vanilla runtime per','MIA mlp', 'MIA svm','MIA ranfor', 'MIA logi', 'MIA ada', 'MIA confidence mse','Epsilon','Delta']\n",
    "    minn = ['Epsilon','Delta']\n",
    "    res = {}\n",
    "   \n",
    "    for k in maxx:\n",
    "        res[k] = cora.max()[k]\n",
    "    for k in meann:\n",
    "        res[k] = cora.mean(numeric_only=True)[k]\n",
    "    for k in minn:\n",
    "        res[k] = cora.mean(numeric_only=True)[k]\n",
    "    return res\n",
    "    \n",
    "df = pd.read_csv('/mnt/ssd1/sunyifan/WorkStation/dpuf/my_DPGCN/ress/cora_ns1_nb1_cluster.csv',index_col=0)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 638,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Vanilla test acc': 0.7521,\n",
       " 'Vanilla test f1': 0.7521,\n",
       " 'Vanilla runtime per': 0.21726363636363633,\n",
       " 'MIA mlp': 0.5046272727272727,\n",
       " 'MIA svm': 0.5177545454545456,\n",
       " 'MIA ranfor': 0.5016454545454546,\n",
       " 'MIA logi': 0.5147,\n",
       " 'MIA ada': 0.5016363636363635,\n",
       " 'MIA confidence mse': 0.5863636363636363,\n",
       " 'Epsilon': 86.9397,\n",
       " 'Delta': 0.0033}"
      ]
     },
     "execution_count": 638,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_res_rdp(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mygcn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
