{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Dataset', 'Vanilla train acc', 'Vanilla train loss',\n",
       "       'Vanilla train f1', 'Vanilla test acc', 'Vanilla test loss',\n",
       "       'Vanilla test f1', 'Shadow train acc', 'Shadow train loss',\n",
       "       'Shadow train f1', 'Shadow test acc', 'Shadow test loss',\n",
       "       'Shadow test f1', 'MIA subsample rate', 'MIA mlp', 'MIA svm',\n",
       "       'MIA ranfor', 'MIA logi', 'MIA ada', 'MIA confidence mse',\n",
       "       'MIA confidence thr', 'MIA seed', 'Vanilla runtime per',\n",
       "       'Shadow runtime per', 'Epsilon', 'Delta', 'Dp', 'Rdp', 'Ldp',\n",
       "       'Norm bound', 'Noise scale', 'Sampler', 'Sampler batchsize',\n",
       "       'Occurance k', 'Cluster numparts', 'Saint rootnodes',\n",
       "       'Saint samplecoverage', 'Saint walklenth', 'Epochs', 'Shadow epochs',\n",
       "       'Num val', 'Num test', 'Layers', 'Hidden dims', 'Learning rate',\n",
       "       'Shadow learning rate', 'Dropout', 'Activation', 'Early stopping',\n",
       "       'Patience', 'Optim type'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from torch_geometric.data import Data,Dataset\n",
    "\n",
    "total_parms_new = pd.read_csv('/mnt/ssd1/sunyifan/WorkStation/dpuf/my_DPGCN/total_params_new.csv',index_col=0)\n",
    "total_parms = pd.read_csv('/mnt/ssd1/sunyifan/WorkStation/dpuf/my_DPGCN/total_params.csv',index_col=0)\n",
    "\n",
    "from typing import List, Literal, Sequence, Union\n",
    "\n",
    "\n",
    "dataset_list = ['cora','citeseer','pubmed','computers','photo','cs','physics','reddit','github','flickr','lastfmasia']\n",
    "\n",
    "def get_datasetwise_df(df:pd.DataFrame,name=None):\n",
    "    if not name:\n",
    "        dflist = []\n",
    "        for i in ['cora','citeseer','pubmed','computers','photo','cs','physics','reddit','github','flickr','lastfmasia']:\n",
    "            dflist.append(df[df['Dataset']==i])\n",
    "        return dflist\n",
    "    else:\n",
    "        return df[df['Dataset']==name]\n",
    "\n",
    "def baseline(df:pd.DataFrame):\n",
    "    clean_adam = df[(df['Dp']==False)&(df['Rdp']==True)&(df['Ldp']==False)&(df['Optim type']=='adam')]\n",
    "    clean_sgd = df[(df['Dp']==False)&(df['Rdp']==True)&(df['Ldp']==False)&(df['Optim type']=='sgd')]\n",
    "    return clean_adam,clean_sgd\n",
    "\n",
    "def rdp_drop(df:pd.DataFrame):\n",
    "    labels = ['Vanilla train acc','Vanilla train loss','Vanilla train f1',\n",
    "          'Shadow train acc', 'Shadow train loss',\n",
    "       'Shadow train f1', 'Shadow test acc', 'Shadow test loss',\n",
    "       'Shadow test f1', 'MIA subsample rate','MIA seed','Shadow runtime per', 'Num val', 'Num test', 'Layers', 'Hidden dims','Activation','Shadow learning rate','Dropout','Sampler', 'Sampler batchsize','Dp','Rdp','Ldp']\n",
    "    return df.drop(columns=labels)\n",
    "\n",
    "def get_res_rdp(cora:pd.DataFrame):\n",
    "    maxx = ['Vanilla test acc','Vanilla test f1']\n",
    "    meann = ['Vanilla runtime per','MIA mlp', 'MIA svm','MIA ranfor', 'MIA logi', 'MIA ada', 'MIA confidence mse','Epsilon','Delta']\n",
    "    minn = ['Epsilon','Delta']\n",
    "    res = {}\n",
    "   \n",
    "    for k in maxx:\n",
    "        res[k] = cora.max()[k]\n",
    "    for k in meann:\n",
    "        res[k] = cora.mean(numeric_only=True)[k]\n",
    "    for k in minn:\n",
    "        res[k] = cora.min()[k]\n",
    "    return res\n",
    "    \n",
    "\n",
    "total_parms_new.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'cora','citeseer','pubmed','computers','photo','cs','physics','reddit','github','flickr','lastfmasia'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = get_datasetwise_df(total_parms)\n",
    "cora = dfs[0]\n",
    "cora_adam,cora_sgd = baseline(cora)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50         cluster\n",
       "270       saint_rw\n",
       "530     saint_node\n",
       "930       neighbor\n",
       "1000     occurance\n",
       "Name: Sampler, dtype: object"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cora_adam['Sampler'].drop_duplicates()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mygcn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
